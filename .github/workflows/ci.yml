name: MLflow Pipeline CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual trigger

jobs:
  mlflow-pipeline-ci:
    runs-on: ubuntu-latest
    
    steps:
    # ==========================================
    # STAGE 1: Environment Setup
    # ==========================================
    - name: "Stage 1: Checkout Repository"
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better debugging
    
    - name: "Stage 1: Set up Python 3.9"
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'  # Cache pip dependencies
    
    - name: "Stage 1: Install Dependencies"
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        echo "âœ… Dependencies installed successfully"
    
    - name: "Stage 1: Verify Environment"
      run: |
        echo "ðŸ” Environment Verification:"
        python --version
        pip list | grep -E "(mlflow|scikit-learn|pandas|numpy|dvc)"
        echo "âœ… Environment setup completed"
    
    # ==========================================
    # STAGE 2: Pipeline Validation
    # ==========================================
    - name: "Stage 2: Prepare Test Data"
      run: |
        echo "ðŸ“Š Setting up lightweight test data..."
        mkdir -p data
        python -c "
        import pandas as pd
        import numpy as np
        from sklearn.datasets import fetch_california_housing
        
        # Create lightweight test dataset (1000 samples instead of 20k)
        housing = fetch_california_housing()
        df = pd.DataFrame(housing.data, columns=housing.feature_names)
        df['target'] = housing.target
        
        # Convert target to categorical for classification
        df['target'] = pd.cut(df['target'], bins=3, labels=['Low', 'Medium', 'High'])
        
        # Save reduced dataset
        df.head(1000).to_csv('data/raw_data.csv', index=False)
        print('âœ… Test data prepared: 1000 samples')
        "
    
    - name: "Stage 2: Initialize DVC (Lightweight)"
      run: |
        echo "ðŸ”§ Initializing DVC for CI..."
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        
        # Check if .dvc already exists, if not initialize
        if [ ! -d ".dvc" ]; then
          dvc init --no-scm
          echo "âœ… DVC initialized"
        else
          echo "âœ… DVC already initialized, skipping"
        fi
    
    - name: "Stage 2: Verify Pipeline Components"
      run: |
        echo "ðŸ” Verifying pipeline components exist..."
        
        # Check required files
        [ -f "src/pipeline.py" ] && echo "âœ… Pipeline script found" || { echo "âŒ Pipeline script missing"; exit 1; }
        [ -f "components/data_extraction.py" ] && echo "âœ… Data extraction component found" || echo "âš ï¸ Data extraction component missing"
        [ -f "components/data_preprocessing.py" ] && echo "âœ… Data preprocessing component found" || echo "âš ï¸ Data preprocessing component missing"
        [ -f "components/model_training.py" ] && echo "âœ… Model training component found" || echo "âš ï¸ Model training component missing"
        [ -f "components/model_evaluation.py" ] && echo "âœ… Model evaluation component found" || echo "âš ï¸ Model evaluation component missing"
        
        echo "âœ… Component verification completed"
    
    - name: "Stage 2: Validate MLflow Pipeline Execution"
      run: |
        echo "ðŸ”„ Running MLflow Pipeline Validation..."
        
        # Create necessary directories with proper permissions
        mkdir -p mlruns data/extracted data/processed models evaluation
        chmod -R 777 mlruns data models evaluation
        
        # Set environment variables for MLflow
        export MLFLOW_TRACKING_URI="file://$(pwd)/mlruns"
        export PYTHONPATH="$(pwd):$(pwd)/src:$PYTHONPATH"
        
        # Verify MLflow configuration
        python -c "
        import os
        import mlflow
        mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))
        print(f'MLflow Tracking URI: {mlflow.get_tracking_uri()}')
        # Test basic MLflow functionality
        client = mlflow.tracking.MlflowClient()
        print('MLflow client initialized successfully')
        "
        
        # Run pipeline with environment variables and timeout protection
        timeout 10m env MLFLOW_TRACKING_URI="file://$(pwd)/mlruns" PYTHONPATH="$(pwd):$(pwd)/src" python src/pipeline.py || {
          echo "âŒ Pipeline execution failed or timed out"
          echo "ðŸ“‹ Debug Information:"
          echo "Working directory: $(pwd)"
          echo "MLflow Tracking URI: $MLFLOW_TRACKING_URI"
          echo "Python Path: $PYTHONPATH"
          ls -la mlruns/ 2>/dev/null || echo "mlruns directory not found or not accessible"
          ls -la data/ 2>/dev/null || echo "data directory not found"
          ls -la src/ 2>/dev/null || echo "src directory not found"
          exit 1
        }
        
        echo "âœ… MLflow pipeline executed successfully"
    
    - name: "Stage 2: Validate MLflow Tracking"
      run: |
        echo "ðŸ“Š Validating MLflow Tracking..."
        export MLFLOW_TRACKING_URI="file://$(pwd)/mlruns"
        
        python -c "
        import os
        import mlflow
        
        # Set tracking URI from environment
        mlflow.set_tracking_uri(os.environ.get('MLFLOW_TRACKING_URI'))
        client = mlflow.tracking.MlflowClient()
        
        # Check experiments exist
        experiments = client.search_experiments()
        print(f'Found {len(experiments)} experiments')
        
        if len(experiments) == 0:
            print('âš ï¸ No experiments found, but pipeline completed - this might be expected in CI')
        else:
            # Check for main orchestration experiment
            main_exp = None
            for exp in experiments:
                print(f'Experiment: {exp.name}')
                if 'component_pipeline_orchestration' in exp.name:
                    main_exp = exp
                    break
            
            if main_exp is not None:
                # Check runs exist
                runs = client.search_runs([main_exp.experiment_id])
                print(f'Found {len(runs)} runs in main experiment')
                
                if len(runs) > 0:
                    latest_run = runs[0]
                    print(f'Latest run status: {latest_run.info.status}')
                    print(f'Latest run ID: {latest_run.info.run_id}')
                    
                    # Validate metrics exist
                    if latest_run.data.metrics:
                        print(f'Metrics found: {list(latest_run.data.metrics.keys())}')
                    else:
                        print('âš ï¸ No metrics found in latest run')
                else:
                    print('âš ï¸ No runs found in main experiment')
            else:
                print('âš ï¸ Main pipeline experiment not found, checking all experiments:')
                for exp in experiments:
                    runs = client.search_runs([exp.experiment_id])
                    print(f'  {exp.name}: {len(runs)} runs')
        
        print('âœ… MLflow tracking validation completed')
        "
    
    - name: "Stage 2: Verify Component Outputs"
      run: |
        echo "ðŸ” Verifying pipeline outputs..."
        
        # Check data files
        [ -f "data/extracted/extracted_data.csv" ] && echo "âœ… Data extraction output found" || echo "âŒ Data extraction output missing"
        [ -d "data/processed" ] && echo "âœ… Data processing outputs found" || echo "âŒ Data processing outputs missing"
        [ -f "models/random_forest_model.pkl" ] && echo "âœ… Model output found" || echo "âŒ Model output missing"
        [ -d "evaluation" ] && echo "âœ… Evaluation outputs found" || echo "âŒ Evaluation outputs missing"
        
        echo "âœ… Component outputs verification completed"
    
    # ==========================================
    # STAGE 3: Tests (Unit Testing)
    # ==========================================
    - name: "Stage 3: Install Testing Dependencies"
      run: |
        pip install pytest pytest-cov
        echo "âœ… Testing dependencies installed"
    
    - name: "Stage 3: Create Unit Tests"
      run: |
        echo "ðŸ“ Creating unit tests..."
        mkdir -p tests
        
        # Create test file for pipeline components
        cat > tests/test_pipeline_components.py << 'EOF'
        """
        Unit tests for MLflow pipeline components
        """
        import pytest
        import pandas as pd
        import numpy as np
        import os
        import sys
        from pathlib import Path
        
        # Add src to path
        sys.path.append(str(Path(__file__).parent.parent / 'src'))
        
        def test_data_loading():
            """Test that raw data can be loaded successfully"""
            assert os.path.exists('data/raw_data.csv'), "Raw data file should exist"
            
            df = pd.read_csv('data/raw_data.csv')
            assert not df.empty, "Dataset should not be empty"
            assert 'target' in df.columns, "Target column should exist"
            print("âœ… Data loading test passed")
        
        def test_data_preprocessing_output():
            """Test that preprocessing generates expected outputs"""
            expected_files = [
                'data/processed/X_train.csv',
                'data/processed/X_test.csv', 
                'data/processed/y_train.csv',
                'data/processed/y_test.csv'
            ]
            
            for file in expected_files:
                if os.path.exists(file):
                    df = pd.read_csv(file)
                    assert not df.empty, f"{file} should not be empty"
            
            print("âœ… Data preprocessing test passed")
        
        def test_model_output():
            """Test that model training generates expected outputs"""
            import joblib
            
            if os.path.exists('models/random_forest_model.pkl'):
                model = joblib.load('models/random_forest_model.pkl')
                assert hasattr(model, 'predict'), "Model should have predict method"
                print("âœ… Model output test passed")
            else:
                print("âš ï¸ Model file not found, skipping model test")
        
        def test_mlflow_experiments():
            """Test that MLflow experiments are created"""
            import mlflow
            
            client = mlflow.tracking.MlflowClient()
            experiments = client.search_experiments()
            
            assert len(experiments) > 0, "Should have at least one experiment"
            
            exp_names = [exp.name for exp in experiments]
            expected_experiments = [
                'component_pipeline_orchestration',
                'pipeline_data_extraction', 
                'pipeline_data_preprocessing',
                'pipeline_model_training'
            ]
            
            found_experiments = [name for name in expected_experiments if any(name in exp_name for exp_name in exp_names)]
            assert len(found_experiments) > 0, f"Should find pipeline experiments. Found: {exp_names}"
            
            print(f"âœ… MLflow experiments test passed. Found: {found_experiments}")
        EOF
        
        echo "âœ… Unit tests created"
    
    - name: "Stage 3: Run Unit Tests"
      run: |
        echo "ðŸ§ª Running unit tests..."
        pytest tests/ -v --tb=short --disable-warnings
        echo "âœ… Unit tests completed"
    
    - name: "Stage 3: Generate Test Coverage Report"
      run: |
        echo "ðŸ“Š Generating test coverage..."
        pytest tests/ --cov=src --cov-report=term-missing
        echo "âœ… Test coverage analysis completed"
    
    # ==========================================
    # FINAL: Cleanup and Summary
    # ==========================================
    - name: "Final: Pipeline Summary"
      run: |
        echo "ðŸ“‹ CI Pipeline Summary:"
        echo "=========================="
        echo "âœ… Stage 1: Environment Setup - COMPLETED"
        echo "âœ… Stage 2: Pipeline Validation - COMPLETED" 
        echo "âœ… Stage 3: Unit Testing - COMPLETED"
        echo ""
        echo "ðŸŽ‰ MLflow CI Pipeline executed successfully!"
        echo "All stages completed without errors."
    
    - name: "Final: Upload Artifacts"
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: mlflow-pipeline-artifacts
        path: |
          mlruns/
          models/
          data/processed/
          evaluation/
        retention-days: 5